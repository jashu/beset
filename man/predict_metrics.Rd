% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prediction_metrics.R
\name{predict_metrics}
\alias{predict_metrics}
\title{Prediction Metrics}
\usage{
predict_metrics(object, test_data = NULL)
}
\arguments{
\item{object}{A model object of class "glm", "negbin", or "zeroinfl".}

\item{test_data}{A data frame containing new data for the response and all
predictors that were used to fit the model \code{object}.}
}
\value{
A list giving the deviance, mean absolute error, mean cross entropy,
mean squared error, and deviance R-squared between model predictions and 
actual observations.
}
\description{
Calculate deviance, mean absolute error, mean cross entropy, mean squared 
error, and R-squared (as fraction of deviance explained) to measure how well
a generalized linear model predicts test data.
}
\details{
\code{predict_metrics} uses a generalized linear model previously fit to a
training set to predict responses for a test set. It then computes the 
residual sum of squares and the log-likelihood of the predicted responses, as
well as the log-likelihoods for the corresponding saturated model (a model 
with one free parameter per observation) and null model (a model with only an
intercept) fit to the true responses. These quantities are used to derive the
metrics described below.
}
\section{Mean squared error (MSE) and mean cross entropy (MCE)}{

MSE is the average of the squared difference between each predicted response
\eqn{ŷ} and the actual observed response \eqn{y}. MCE is an analagous
information-theoretic quantity that averages the negative logs of the
probability density functions for \eqn{ŷ} evaluated at \eqn{y}.

Note that cross entropy simply parameterizes prediction error in terms of
relative likelihood; if applied to the training set instead of the test set,
MCE equals the negative log-likelihood of the model divided by the number of
observations. (For logistic regression, this is also known as "log-loss".)
Given that GLMs minimize the negative log-likelihood, cross entropy defines a
unified loss function for both fitting and cross-validating GLMs.
}

\section{R-squared and deviance explained}{

The prediction R-squared provides a metric analagous to R-squared, except
instead of describing how well a model fits the original training data, it
describes how well the model predicts independent test data. Instead of using
the traditional formula, \eqn{1 - RSS / TSS}, where RSS corresponds to
the sum of the squared residuals, \eqn{(y - ŷ)^2}, and TSS is the total sum
of squares, it is calculated as \eqn{1 - dev_PRED / dev_NULL}, where
\eqn{dev_PRED} is the deviance for the model prediction and \eqn{dev_NULL} is
the deviance for the null model. This yields a quantity equivalent to the
traditional R-squared formula for linear models with normal error
distributions, but a different quantity for exponential family regression
models (e.g., logistic regression) that preserves the interpretation of
R-squared as the fraction of uncertainty explained (Cameron & Windmeijer,
1997). (See \code{\link{r2d}} for more details.)

Note that the prediction R-squared can be negative if overfitting is severe;
i.e., when predicting new data, the model performs worse than if one were to
always predict the mean response.
}

\section{Supported Probability Distributions}{

Currently the following "families" are supported: Gaussian, binomial,
Poisson, negative binomial, zero-inflated Poisson, and zero-inflated negative
binomial.
}

\seealso{
\code{\link{r2d}}, \code{\link[stats]{logLik}},
\code{\link{deviance.zeroinfl}}
}
